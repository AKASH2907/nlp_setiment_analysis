{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Algorithms\n",
    "from keras.layers import Flatten,LSTM,Conv1D,Dropout,Bidirectional, GRU, RNN, Concatenate\n",
    "from keras.utils import plot_model, np_utils\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv1D, RNN, LSTM, GlobalAveragePooling1D, Dense\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8512\n",
      "8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "topics = train.topic.unique().tolist()\n",
    "\n",
    "l = list(range(21))\n",
    "labels = {i : topics[i] for i in range(0, len(topics))}\n",
    "d = dict(zip(topics, l))\n",
    "# print(labels)\n",
    "# print(d)\n",
    "\n",
    "train['labels'] = train[\"topic\"].apply(lambda x: d[x])\n",
    "all_data = pd.concat([train, test], ignore_index=False)\n",
    "titles = []\n",
    "reviews= []\n",
    "\n",
    "for i in all_data['Review Title']:\n",
    "    titles.append(i)\n",
    "\n",
    "for line in all_data['Review Text']:\n",
    "    reviews.append(line)\n",
    "\n",
    "print(len(titles))\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5959 2553\n",
      "(5959, 21)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "titles_clean = preprocess_reviews(titles)\n",
    "reviews_clean = preprocess_reviews(reviews)\n",
    "\n",
    "\n",
    "X_train = titles_clean[:train.shape[0]]\n",
    "X_test = titles_clean[train.shape[0]:]\n",
    "print(len(X_train), len(X_test))\n",
    "X_trainj = reviews_clean[:train.shape[0]]\n",
    "X_testj = reviews_clean[train.shape[0]:]\n",
    "Y = []\n",
    "\n",
    "for i in train['labels']:\n",
    "    Y+=[i]\n",
    "\n",
    "Y = np.asarray(Y)\n",
    "Y = np_utils.to_categorical(Y, 21)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2547\n"
     ]
    }
   ],
   "source": [
    "freq_words_state=400\n",
    "\n",
    "tokenizer_state = Tokenizer(num_words=freq_words_state)\n",
    "tokenizer_state.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "X_train = tokenizer_state.texts_to_sequences(X_train)\n",
    "# X_val = tokenizer_state.texts_to_sequences(X_val)\n",
    "X_test = tokenizer_state.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "vocab_size_state = len(tokenizer_state.word_index) + 1\n",
    "print(vocab_size_state)\n",
    "maxlen_s = 30\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen_s, padding='post')\n",
    "# X_val = pad_sequences(X_val, maxlen=maxlen_s, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen_s, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 82 206 114  44  88   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[895])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.50d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0810 16:44:34.717592 140455335413568 deprecation_wrapper.py:119] From /home/akash/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2547, 50)\n",
      "<keras.layers.embeddings.Embedding object at 0x7fbe027d7550>\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_s = zeros((vocab_size_state, 50))\n",
    "for word, index in tokenizer_state.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_s[index] = embedding_vector\n",
    "    \n",
    "print(embedding_matrix_s.shape)\n",
    "\n",
    "embedding_layer_s = Embedding(vocab_size_state, 50, weights=[embedding_matrix_s],\n",
    "                              input_length=maxlen_s , trainable=False)\n",
    "print(embedding_layer_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5959\n"
     ]
    }
   ],
   "source": [
    "freq_words_justify=2000\n",
    "\n",
    "tokenizer_justify = Tokenizer(num_words=freq_words_justify)\n",
    "tokenizer_justify.fit_on_texts(X_trainj)\n",
    "\n",
    "X_trainj = tokenizer_justify.texts_to_sequences(X_trainj)\n",
    "# X_valj = tokenizer_justify.texts_to_sequences(X_valj)\n",
    "X_testj = tokenizer_justify.texts_to_sequences(X_testj)\n",
    "print(len(X_trainj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8763\n"
     ]
    }
   ],
   "source": [
    "vocab_size_justify = len(tokenizer_justify.word_index) + 1\n",
    "print(vocab_size_justify)\n",
    "maxlen_j = 150\n",
    "\n",
    "X_trainj = pad_sequences(X_trainj, maxlen=maxlen_j, padding='post')\n",
    "# X_valj = pad_sequences(X_valj, maxlen=maxlen_j, padding='post')\n",
    "X_testj = pad_sequences(X_testj, maxlen=maxlen_j, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14,   72,    4,    3,    1,  292,    4,  538,   92,  140,   50,\n",
       "         34,   27,   76,  192,    5,  219,   23,   11,    5,  514,  161,\n",
       "        132,  199,    9,   89,   10,    3,  111,   15,  304,  229,   16,\n",
       "          2, 1062,  406,   34,    9,   43,  499,  406,   29,    5,  775,\n",
       "         35,   10,  223,  107,   21,   89,  185,    2,   26,    1,  262,\n",
       "         19,    7,   35,   28,    2,    6,  773,   43, 1470,  183,    8,\n",
       "        525,   11,    2,  114,  161,   29,    5,  137,   35,   80,   83,\n",
       "         63,    5, 1063,    8,  183,   66,  143,  209,  311,   11,    2,\n",
       "        208,  453,  535,   20,  525,   62,  365,  189,   11,   25,  111,\n",
       "          2,  263,   16,    1,   17,   65,   58,   30,    1,   73, 1348,\n",
       "         12, 1235,    3,  118,    1,   62,    9,  182,   81,  406,  882,\n",
       "        189,  311,   19,  274,    4,  538,  140,    3,   59,    5,   51,\n",
       "        533,   13,    7,   10,    9,    5,  533,   42,   12,  531,    6,\n",
       "         14,   21,   25,  814,  175,   12,  140], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainj[167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8763, 50)\n",
      "<keras.layers.embeddings.Embedding object at 0x7fbe0265cc50>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_matrix_j = zeros((vocab_size_justify, 50))\n",
    "for word, index in tokenizer_justify.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_j[index] = embedding_vector\n",
    "    \n",
    "print(embedding_matrix_j.shape)\n",
    "\n",
    "embedding_layer_j = Embedding(vocab_size_justify, 50, weights=[embedding_matrix_j],\n",
    "                              input_length=maxlen_j , trainable=False)\n",
    "print(embedding_layer_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.11891   ,  0.15255   , -0.082073  , ..., -0.57511997,\n",
       "        -0.26671001,  0.92120999],\n",
       "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
       "        -0.11514   , -0.78580999],\n",
       "       ...,\n",
       "       [ 0.080652  , -0.64354002,  0.12345   , ...,  0.13384999,\n",
       "         0.1115    , -0.56487   ],\n",
       "       [ 0.28990999, -0.95266002, -0.43441001, ..., -0.41021001,\n",
       "         1.71749997, -0.39603001],\n",
       "       [ 0.34303001, -0.18794   ,  0.20774999, ...,  0.23029999,\n",
       "         0.31827   , -0.40224999]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0810 16:44:49.147285 140455335413568 deprecation_wrapper.py:119] From /home/akash/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0810 16:44:49.149177 140455335413568 deprecation_wrapper.py:119] From /home/akash/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0810 16:44:49.154724 140455335413568 deprecation_wrapper.py:119] From /home/akash/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0810 16:44:49.155203 140455335413568 deprecation_wrapper.py:119] From /home/akash/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0810 16:44:49.233893 140455335413568 deprecation_wrapper.py:119] From /home/akash/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 30, 50)            127350    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21)                31521     \n",
      "=================================================================\n",
      "Total params: 158,871\n",
      "Trainable params: 31,521\n",
      "Non-trainable params: 127,350\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "seq_input = Input(shape=(maxlen_s, ), dtype='int32')\n",
    "x = embedding_layer_s(seq_input)\n",
    "x = Flatten()(x)\n",
    "x = Dense(21, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=seq_input, outputs=x)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, Y, batch_size=128, epochs=5, verbose=1)\n",
    "\n",
    "# loss, acc= model.evaluate(X_train, Y, verbose=1)\n",
    "# print(acc)\n",
    "# t = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(t.shape)\n",
    "# print(t[1])\n",
    "# result = np.where(t[1] == np.amax(t[1]))\n",
    "# result = np.argmax(t[1])\n",
    "# print(result)\n",
    "# result = []\n",
    "# for i in range(t.shape[0]):\n",
    "#     result+=[np.argmax(t[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(result))\n",
    "# final = []\n",
    "# for i in result:\n",
    "#     final.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 150, 50)           438150    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 128)          32128     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 142, 256)          164096    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21)                5397      \n",
      "=================================================================\n",
      "Total params: 639,771\n",
      "Trainable params: 201,621\n",
      "Non-trainable params: 438,150\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "seq_input = Input(shape=(maxlen_j, ), dtype='int32')\n",
    "x = embedding_layer_j(seq_input)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(21, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=seq_input, outputs=x)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5959/5959 [==============================] - 6s 937us/step - loss: 1.3845 - acc: 0.5482\n",
      "Epoch 2/10\n",
      "5959/5959 [==============================] - 6s 1ms/step - loss: 1.2924 - acc: 0.5655\n",
      "Epoch 3/10\n",
      "5959/5959 [==============================] - 6s 976us/step - loss: 1.1982 - acc: 0.5877\n",
      "Epoch 4/10\n",
      "5959/5959 [==============================] - 5s 862us/step - loss: 1.1201 - acc: 0.5971\n",
      "Epoch 5/10\n",
      "5959/5959 [==============================] - 5s 801us/step - loss: 1.0608 - acc: 0.6147\n",
      "Epoch 6/10\n",
      "5959/5959 [==============================] - 5s 782us/step - loss: 1.0184 - acc: 0.6251\n",
      "Epoch 7/10\n",
      "5959/5959 [==============================] - 5s 771us/step - loss: 0.9456 - acc: 0.6283\n",
      "Epoch 8/10\n",
      "5959/5959 [==============================] - 5s 825us/step - loss: 0.8934 - acc: 0.6400\n",
      "Epoch 9/10\n",
      "5959/5959 [==============================] - 5s 785us/step - loss: 0.8834 - acc: 0.6389\n",
      "Epoch 10/10\n",
      "5959/5959 [==============================] - 5s 789us/step - loss: 0.8525 - acc: 0.6426\n",
      "5959/5959 [==============================] - 2s 253us/step\n",
      "0.6939083739082412\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_trainj, Y, batch_size=128, epochs=10, verbose=1)\n",
    "\n",
    "loss, acc= model.evaluate(X_trainj, Y, verbose=1)\n",
    "print(acc)\n",
    "t = model.predict(X_testj)\n",
    "\n",
    "result = []\n",
    "for i in range(t.shape[0]):\n",
    "    result+=[np.argmax(t[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for i in result:\n",
    "    final.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'Review Text': test['Review Text'],\n",
    "    'Review Title':test['Review Title'],\n",
    "    'topic': final\n",
    "})\n",
    "\n",
    "submission.to_csv('submit_dl2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
